{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b9e7987-4171-4af6-b1aa-34f04dd6f269",
   "metadata": {},
   "source": [
    "### Working aiothhtp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e6f980-3b77-489c-a64f-e78b5a24cbd8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### First, create offsets of urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd33ab6a-f80b-4e14-bb0e-bff40c276ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store new created offset urls\n",
    "all_urls = []\n",
    "# Base url that serves as preffix of all offset urls\n",
    "offset_base = \"https://sofifa.com/players?offset=\"\n",
    "# Loop to append offset number at very end of the offset base\n",
    "# Define a n to set range of when to stop adding\n",
    "n = 20040 # n should be large enough\n",
    "for offset in range(0, n, 60):\n",
    "    all_urls.append(offset_base + str(offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1905eba-b878-46b6-acc5-ac65785d4034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sofifa.com/players?offset=0',\n",
       " 'https://sofifa.com/players?offset=60',\n",
       " 'https://sofifa.com/players?offset=120',\n",
       " 'https://sofifa.com/players?offset=180',\n",
       " 'https://sofifa.com/players?offset=240']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prints first 5 links \n",
    "all_urls[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3a35b4-4c13-4d10-93f8-1a76a728c530",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Next, try assgining number of offset links for each scraper.\n",
    "For example:\n",
    "    \n",
    "   Let $S_{x}$ be scraper $x$, and let all_urls = $O$ s.t. :\n",
    "       \n",
    "   - $S_1$ scraps O[0:10]\n",
    "   - $S_2$ scraps O[11:20]\n",
    "   - ...\n",
    "   - $S_x$ scraps O[x-10 : x]\n",
    "\n",
    "This way we could assign to $x$ numbers of scrapers certain numbers of offset links evenly distributed, and each scraper just parse the links they have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "760f117b-d4d0-41b4-b2f5-9c0768842181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import aiohttp\n",
    "from bloom_filter import BloomFilter\n",
    "import lxml.html as lx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cee9354-b65a-48d9-9520-8416c40d0d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be run if in interactive kernel like jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "440318f2-dad4-41ce-b50d-1be3216c754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate list to store hyperlinks of each individual player\n",
    "player_links = []\n",
    "# Instatiate bloomfilter to check duplicates of player links\n",
    "bloom = BloomFilter(max_elements=30000, error_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d8a97-38c5-4cdb-bc13-1955d0a2b7ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Function to download (change this word) each url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fce83b20-6e32-4366-acca-c60f97a7f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        url = await fetch(session, url)\n",
    "        await parse_offset(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c0c37f-2055-4d4f-b7a2-b42ea2f0585f",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Function to fetch and requests to GET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aed77351-db69-47bb-9760-2aa4a44e8dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        assert response.status == 200\n",
    "        return await response.text(encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9efd02-92ec-46f6-92b6-a0879874964e",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Function to parse each url (This case our offset link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f963c-e1e0-4692-8526-9e4a9a5c0b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def parse_offset(url):\n",
    "    doc = lx.fromstring(url)\n",
    "    out = listing()\n",
    "    for path in out:\n",
    "        href = doc.xpath(path)[0]\n",
    "        # This checks the version (2 digits) of the player \n",
    "        ver = href.split(sep=\"/\")[4][0:2]\n",
    "        # If the player is from older version that isnt 22 then ignore\n",
    "        if ver != \"22\":\n",
    "            continue\n",
    "        # Defines prefix of the p_url that shares everyone\n",
    "        base = \"https://sofifa.com\"\n",
    "        p_url = base + href\n",
    "        # Checks if this player if already contains in our bloom set to check duplicate\n",
    "        if bloom.__contains__(p_url):\n",
    "            # print(f\"This url is duplicated {p_url}\")\n",
    "            continue\n",
    "        player_links.append(p_url)\n",
    "        bloom.add(p_url)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2393bcf3-1a80-4cd8-8dba-0113c01d8034",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### Helper function for parse_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0bf7db3-01ee-4c8c-912d-6c4ecc909840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listing():\n",
    "    out = []\n",
    "    ref = '//*[@id=\"body\"]/div[1]/div/div[2]/div/table/tbody/tr[i]/td[2]/a[1]/@href'\n",
    "    old = ref.split(sep=\"/\")[9]\n",
    "    for i in range(60):\n",
    "        new = f\"tr[{i+1}]\"\n",
    "        p = ref.replace(old, new)\n",
    "        out.append(p)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bb976-d6f3-4d55-8cfa-8641aef8f7dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Running the asynchronus process with coroutine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a85310-d027-4bb7-89b0-5537a1f6ff7d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Changes the variable of n to control length of all_urls list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23ee8422-6a06-4ba0-b7ac-00dd10db6314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store new created offset urls\n",
    "all_urls = []\n",
    "# Base url that serves as preffix of all offset urls\n",
    "offset_base = \"https://sofifa.com/players?offset=\"\n",
    "# Loop to append offset number at very end of the offset base\n",
    "# Define a n to set range of when to stop adding\n",
    "n = 120 # n should be large enough\n",
    "for offset in range(0, n, 60):\n",
    "    all_urls.append(offset_base + str(offset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23f83f0-34b1-4489-b38a-667d916a3819",
   "metadata": {},
   "source": [
    "##### Actual running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0871c81c-fb1b-489b-b5f3-c985ec0103ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper():\n",
    "    def __init__(self):\n",
    "        self._player_links = []\n",
    "        self._offset_links = []\n",
    "        self._listing = []\n",
    "        self._bloom = BloomFilter(max_elements=30000, error_rate=0.1)\n",
    "    \n",
    "    def listing(self):\n",
    "        ref = '//*[@id=\"body\"]/div[1]/div/div[2]/div/table/tbody/tr[i]/td[2]/a[1]/@href'\n",
    "        old = ref.split(sep=\"/\")[9]\n",
    "        for i in range(60):\n",
    "            new = f\"tr[{i+1}]\"\n",
    "            p = ref.replace(old, new)\n",
    "            self._listing.append(p)\n",
    "        #return self._listing\n",
    "    \n",
    "    def get_offsets(self, n):\n",
    "        # Base url that serves as preffix of all offset urls\n",
    "        offset_base = \"https://sofifa.com/players?offset=\"\n",
    "        # Loop to append offset number at very end of the offset base\n",
    "        # Define a n to set range of when to stop adding\n",
    "        # n should be large enough\n",
    "        for offset in range(0, n, 60):\n",
    "            self._offset_links.append(offset_base + str(offset))\n",
    "            \n",
    "    async def download(self, url):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            url = await fetch(session, url)\n",
    "            await parse_offset(url)\n",
    "   \n",
    "    async def fetch(self, session, url):\n",
    "        async with session.get(url) as response:\n",
    "            assert response.status == 200\n",
    "            return await response.text(encoding=\"utf-8\")\n",
    "    \n",
    "    async def parse_offset(self, url):\n",
    "        doc = lx.fromstring(url)\n",
    "        assert len(out) != 0\n",
    "        out = self._listing\n",
    "        for path in out:\n",
    "            href = doc.xpath(path)[0]\n",
    "            # This checks the version (2 digits) of the player \n",
    "            ver = href.split(sep=\"/\")[4][0:2]\n",
    "            # If the player is from older version that isnt 22 then ignore\n",
    "            if ver != \"22\":\n",
    "                continue\n",
    "            # Defines prefix of the p_url that shares everyone\n",
    "            base = \"https://sofifa.com\"\n",
    "            p_url = base + href\n",
    "            # Checks if this player if already contains in our bloom set to check duplicate\n",
    "            if self._bloom.__contains__(p_url):\n",
    "                # print(f\"This url is duplicated {p_url}\")\n",
    "                continue\n",
    "            self._player_links.append(p_url)\n",
    "            self._bloom.add(p_url)\n",
    "    \n",
    "    @property\n",
    "    def player_links(self):\n",
    "        return self._player_links\n",
    "    @property\n",
    "    def bloom(self):\n",
    "        return self._bloom\n",
    "    @property\n",
    "    def offset_links(self):\n",
    "        return self._offset_links\n",
    "    @property\n",
    "    def lists(self):\n",
    "        return self._listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "142c818a-980e-46da-a714-5d7f59765c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Using coroutine took: 0.4755258560180664 s\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "print(\"#\" * 20)\n",
    "# starts timer\n",
    "# Use this function to clear existing bloom and player links'\n",
    "sc = Scraper()\n",
    "t1 = time.time()\n",
    "sc.get_offsets(n=120)\n",
    "sc.listing()\n",
    "loop = asyncio.get_event_loop()\n",
    "tasks = [asyncio.ensure_future(sc.download(url)) for url in sc.offset_links]\n",
    "tasks = asyncio.gather(*tasks)\n",
    "loop.run_until_complete(tasks)\n",
    "# ends timer\n",
    "t2 = time.time()\n",
    "print(f\"Using coroutine took: {t2 - t1} s\")\n",
    "print(\"#\" * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d817d1ba-2031-487b-b079-8bf4f2e99ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.player_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7df55c8-e6f9-495e-99a8-2c256e432a51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### helper to clear exisiting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f4de189-91e9-49c1-89bb-07a687a6469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_storage():\n",
    "    # Instatiate list to store hyperlinks of each individual player\n",
    "    player_links = []\n",
    "    # Instatiate bloomfilter to check duplicates of player links\n",
    "    bloom = BloomFilter(max_elements=30000, error_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3ed467-97d7-4608-9cfe-a365cd98a3dc",
   "metadata": {},
   "source": [
    "### Others "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
