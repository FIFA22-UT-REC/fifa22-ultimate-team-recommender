{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b9e7987-4171-4af6-b1aa-34f04dd6f269",
   "metadata": {},
   "source": [
    "### Working aiothhtp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6fb0e8-4124-4929-981b-90b5bb3f4b02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### NOT WORKING CLASS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60896952-7f7d-4a8c-b6ba-c6e20f5225c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper():\n",
    "    def __init__(self):\n",
    "        self._player_links = []\n",
    "        self._offset_links = []\n",
    "        self._listing = []\n",
    "        self._bloom = BloomFilter(max_elements=30000, error_rate=0.1)\n",
    "    \n",
    "    def listing(self):\n",
    "        ref = '//*[@id=\"body\"]/div[1]/div/div[2]/div/table/tbody/tr[i]/td[2]/a[1]/@href'\n",
    "        old = ref.split(sep=\"/\")[9]\n",
    "        for i in range(60):\n",
    "            new = f\"tr[{i+1}]\"\n",
    "            p = ref.replace(old, new)\n",
    "            self._listing.append(p)\n",
    "        #return self._listing\n",
    "    \n",
    "    def get_offsets(self, n):\n",
    "        # Base url that serves as preffix of all offset urls\n",
    "        offset_base = \"https://sofifa.com/players?offset=\"\n",
    "        # Loop to append offset number at very end of the offset base\n",
    "        # Define a n to set range of when to stop adding\n",
    "        # n should be large enough\n",
    "        for offset in range(0, n, 60):\n",
    "            self._offset_links.append(offset_base + str(offset))\n",
    "            \n",
    "    async def download(self, url):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            url = await fetch(session, url)\n",
    "            await parse_offset(url)\n",
    "   \n",
    "    async def fetch(self, session, url):\n",
    "        async with session.get(url) as response:\n",
    "            assert response.status == 200\n",
    "            return await response.text(encoding=\"utf-8\")\n",
    "    \n",
    "    async def parse_offset(self, url):\n",
    "        doc = lx.fromstring(url)\n",
    "        assert len(out) != 0\n",
    "        out = self._listing\n",
    "        for path in out:\n",
    "            href = doc.xpath(path)[0]\n",
    "            # This checks the version (2 digits) of the player \n",
    "            ver = href.split(sep=\"/\")[4][0:2]\n",
    "            # If the player is from older version that isnt 22 then ignore\n",
    "            if ver != \"22\":\n",
    "                continue\n",
    "            # Defines prefix of the p_url that shares everyone\n",
    "            base = \"https://sofifa.com\"\n",
    "            p_url = base + href\n",
    "            # Checks if this player if already contains in our bloom set to check duplicate\n",
    "            if self._bloom.__contains__(p_url):\n",
    "                # print(f\"This url is duplicated {p_url}\")\n",
    "                continue\n",
    "            self._player_links.append(p_url)\n",
    "            self._bloom.add(p_url)\n",
    "    \n",
    "    @property\n",
    "    def player_links(self):\n",
    "        return self._player_links\n",
    "    @property\n",
    "    def bloom(self):\n",
    "        return self._bloom\n",
    "    @property\n",
    "    def offset_links(self):\n",
    "        return self._offset_links\n",
    "    @property\n",
    "    def lists(self):\n",
    "        return self._listing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e6f980-3b77-489c-a64f-e78b5a24cbd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### First, create offsets of urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fd33ab6a-f80b-4e14-bb0e-bff40c276ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store new created offset urls\n",
    "all_urls = []\n",
    "# Base url that serves as preffix of all offset urls\n",
    "offset_base = \"https://sofifa.com/players?offset=\"\n",
    "# Loop to append offset number at very end of the offset base\n",
    "# Define a n to set range of when to stop adding\n",
    "n = 120 # n should be large enough\n",
    "for offset in range(0, n, 60):\n",
    "    all_urls.append(offset_base + str(offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c1905eba-b878-46b6-acc5-ac65785d4034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sofifa.com/players?offset=0', 'https://sofifa.com/players?offset=60']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prints first 5 links \n",
    "all_urls[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3a35b4-4c13-4d10-93f8-1a76a728c530",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Next, try assgining number of offset links for each scraper.\n",
    "For example:\n",
    "    \n",
    "   Let $S_{x}$ be scraper $x$, and let all_urls = $O$ s.t. :\n",
    "       \n",
    "   - $S_1$ scraps O[0:10]\n",
    "   - $S_2$ scraps O[11:20]\n",
    "   - ...\n",
    "   - $S_x$ scraps O[x-10 : x]\n",
    "\n",
    "This way we could assign to $x$ numbers of scrapers certain numbers of offset links evenly distributed, and each scraper just parse the links they have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "760f117b-d4d0-41b4-b2f5-9c0768842181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import aiohttp\n",
    "from bloom_filter import BloomFilter\n",
    "import lxml.html as lx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4cee9354-b65a-48d9-9520-8416c40d0d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be run if in interactive kernel like jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "440318f2-dad4-41ce-b50d-1be3216c754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate list to store hyperlinks of each individual player\n",
    "player_links = []\n",
    "# Instatiate bloomfilter to check duplicates of player links\n",
    "bloom = BloomFilter(max_elements=30000, error_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d8a97-38c5-4cdb-bc13-1955d0a2b7ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Function to download (change this word) each url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fce83b20-6e32-4366-acca-c60f97a7f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        url = await fetch(session, url)\n",
    "        await parse_offset(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c0c37f-2055-4d4f-b7a2-b42ea2f0585f",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Function to fetch and requests to GET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "aed77351-db69-47bb-9760-2aa4a44e8dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        assert response.status == 200\n",
    "        return await response.text(encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9efd02-92ec-46f6-92b6-a0879874964e",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Function to parse each url (This case our offset link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c76f963c-e1e0-4692-8526-9e4a9a5c0b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def parse_offset(url):\n",
    "    doc = lx.fromstring(url)\n",
    "    out = listing()\n",
    "    for path in out:\n",
    "        href = doc.xpath(path)[0]\n",
    "        # This checks the version (2 digits) of the player \n",
    "        ver = href.split(sep=\"/\")[4][0:2]\n",
    "        # If the player is from older version that isnt 22 then ignore\n",
    "        if ver != \"22\":\n",
    "            continue\n",
    "        # Defines prefix of the p_url that shares everyone\n",
    "        base = \"https://sofifa.com\"\n",
    "        p_url = base + href\n",
    "        # Checks if this player if already contains in our bloom set to check duplicate\n",
    "        if bloom.__contains__(p_url):\n",
    "            # print(f\"This url is duplicated {p_url}\")\n",
    "            continue\n",
    "        player_links.append(p_url)\n",
    "        bloom.add(p_url)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2393bcf3-1a80-4cd8-8dba-0113c01d8034",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### Helper function for parse_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f0bf7db3-01ee-4c8c-912d-6c4ecc909840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listing():\n",
    "    out = []\n",
    "    ref = '//*[@id=\"body\"]/div[1]/div/div[2]/div/table/tbody/tr[i]/td[2]/a[1]/@href'\n",
    "    old = ref.split(sep=\"/\")[9]\n",
    "    for i in range(60):\n",
    "        new = f\"tr[{i+1}]\"\n",
    "        p = ref.replace(old, new)\n",
    "        out.append(p)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bb976-d6f3-4d55-8cfa-8641aef8f7dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Running the asynchronus process with coroutine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a85310-d027-4bb7-89b0-5537a1f6ff7d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Changes the variable of n to control length of all_urls list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "23ee8422-6a06-4ba0-b7ac-00dd10db6314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store new created offset urls\n",
    "all_urls = []\n",
    "# Base url that serves as preffix of all offset urls\n",
    "offset_base = \"https://sofifa.com/players?offset=\"\n",
    "# Loop to append offset number at very end of the offset base\n",
    "# Define a n to set range of when to stop adding\n",
    "n = 120 # n should be large enough\n",
    "for offset in range(0, n, 60):\n",
    "    all_urls.append(offset_base + str(offset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23f83f0-34b1-4489-b38a-667d916a3819",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Actual running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "142c818a-980e-46da-a714-5d7f59765c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Using coroutine took: 0.5097517967224121 s\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "print(\"#\" * 20)\n",
    "# starts timer\n",
    "# Use this function to clear existing bloom and player links'\n",
    "t1 = time.time()\n",
    "loop = asyncio.get_event_loop()\n",
    "tasks = [asyncio.ensure_future(download(url)) for url in all_urls]\n",
    "tasks = asyncio.gather(*tasks)\n",
    "loop.run_until_complete(tasks)\n",
    "# ends timer\n",
    "t2 = time.time()\n",
    "print(f\"Using coroutine took: {t2 - t1} s\")\n",
    "print(\"#\" * 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7df55c8-e6f9-495e-99a8-2c256e432a51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### helper to clear exisiting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f4de189-91e9-49c1-89bb-07a687a6469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_storage():\n",
    "    # Instatiate list to store hyperlinks of each individual player\n",
    "    player_links = []\n",
    "    # Instatiate bloomfilter to check duplicates of player links\n",
    "    bloom = BloomFilter(max_elements=30000, error_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28938d6-e72c-4670-82e6-333a46bf43b3",
   "metadata": {},
   "source": [
    "#### Create functions that can parse each player link and get information down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "dd58d0a2-d255-46a0-8bd8-c0ba60157f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sofifa.com/player/246174/harvey-elliott/220059/',\n",
       " 'https://sofifa.com/player/247246/khephren-thuram/220059/',\n",
       " 'https://sofifa.com/player/247497/armel-bella-kotchap/220059/',\n",
       " 'https://sofifa.com/player/239231/marc-cucurella-saseta/220059/',\n",
       " 'https://sofifa.com/player/264309/arda-guler/220059/']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_links[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a0ea6dc7-8529-4135-8b4c-a5d0f95d1203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of links we have\n",
    "len(raw_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3db94722-4b26-44ce-94ea-ebc75739ff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = raw_links[0]\n",
    "l2 = raw_links[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "bddda740-c0bd-4d2c-9091-54ff2d66c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c4d95903-fb80-4adb-959e-9d85e0a1835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_player(link):\n",
    "    resp = requests.get(link)\n",
    "    soup = BS(resp.content, \"lxml\")\n",
    "    #print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "56199121-8895-4798-ba84-6bdd99c52611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to parse xml, 0.8437793254852295\n"
     ]
    }
   ],
   "source": [
    "t3 = time.time()\n",
    "parse_player(l1)\n",
    "print(f\"Time taken to parse xml, {time.time() - t3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2c6e885d-e40a-47a0-9c49-9d48f680ee57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://sofifa.com/player/246174/harvey-elliott/220059/\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f3ed467-97d7-4608-9cfe-a365cd98a3dc",
   "metadata": {},
   "source": [
    "### Others "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
