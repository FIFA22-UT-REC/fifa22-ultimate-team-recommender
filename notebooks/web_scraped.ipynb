{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94ce343-e1c1-4b97-ab2d-6dfa2b918539",
   "metadata": {},
   "source": [
    "## This is a web scrapping for raw data using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c961a9-0f55-46e0-90e0-f78eef5547dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f6de61-0c7e-43e8-9dde-0c717417521b",
   "metadata": {},
   "source": [
    "Set up global variables and Dataframe from pandas to store values that we are going to retrieve/scrap from and use for loop to repeat it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093672d0-db96-4b34-805c-bbbe4bb3e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_scraped = []\n",
    "url = \"https://sofifa.com/players?offset=1\"\n",
    "res = requests.get(url)\n",
    "text = res.text\n",
    "soup = BeautifulSoup(text, \"html.parser\")\n",
    "tbody = soup.find(\"tbody\", {\"class\": \"list\"})\n",
    "trs = tbody.findAll(\"tr\")\n",
    "players_scraped.append([extract_info(tr) for tr in trs])\n",
    "\n",
    "# Convert list of lists to single list\n",
    "flatten = lambda x: list(chain.from_iterable(x))\n",
    "\n",
    "# Convert to df\n",
    "df = pd.DataFrame(flatten(players_scraped))\n",
    "df['value'] = df['value'].apply(convert_into_val)\n",
    "df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6faac143-5889-4245-9b01-464bd47cb45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(tr):\n",
    "    return {\n",
    "        \"name\": tr.select('td.col-name')[0].find(\"a\").get(\"aria-label\"),\n",
    "        \"country\": tr.select('td.col-name')[0].find(\"img\").get(\"title\"),\n",
    "        \"age\": tr.select('td.col.col-ae')[0].text.strip(),\n",
    "        \"overall\": tr.select('td.col.col-oa')[0].text.strip(),\n",
    "        \"potential\": tr.select('td.col.col-pt')[0].text.strip(),\n",
    "        \"club\": tr.select(\"td.col-name\")[1].find(\"a\").text,\n",
    "        # \"height\": tr.select('td.col.col-hi')[0].text.strip(),\n",
    "        # \"weight\": tr.select('td.col.col-wi')[0].text.strip(),\n",
    "        # \"foot\": tr.select('td.col.col-pf')[0].text.strip(),\n",
    "        \"best_position\": tr.select('td.col-name')[0].find(\"span\").text,\n",
    "        \"value\": tr.select('td.col.col-vl')[0].text.strip(),\n",
    "        \"wage\": tr.select('td.col.col-wg')[0].text.strip(),\n",
    "        # \"PAC\": tr.select('td.col.col-pac')[0].text.strip(),\n",
    "        # \"SHO\": tr.select('td.col.col-sho')[0].text.strip(),\n",
    "        # \"PAS\": tr.select('td.col.col-pas')[0].text.strip(),\n",
    "        # \"DRI\": tr.select('td.col.col-dir')[0].text.strip(),\n",
    "        # \"DEF\": tr.select('td.col.col-def')[0].text.strip(),\n",
    "        # \"PHY\": tr.select('td.col.col-phy')[0].text.strip()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83e536a-5ff1-4ad0-88e5-45b6748ed3fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://sofifa.com/players?offset=1\"\n",
    "columns = [\"ID\", \"Name\", \"Age\", \"Position\", \"Nationality\", \"Overall\",\n",
    "          \"Potential\", \"Club\", \"Value\", \"Wage\"]\n",
    "data = pd.DataFrame(columns = columns)\n",
    "\n",
    "src_code = requests.get(url)\n",
    "plain_text = src_code.text\n",
    "soup = BeautifulSoup(plain_text,\"html.parser\")\n",
    "tbody = soup.find(\"tbody\")\n",
    "trs = tbody.findAll(\"tr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c10203-7520-4419-b443-03c28e6cd05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for 2\r"
     ]
    }
   ],
   "source": [
    "base_url = \"https://sofifa.com/players?offset=\"\n",
    "columns = [\"ID\", \"Name\", \"Age\", \"Position\", \"Nationality\", \"Overall\",\n",
    "          \"Potential\", \"Club\", \"Value\", \"Wage\"]\n",
    "\n",
    "data = pd.DataFrame(columns = columns)\n",
    "p_urls = []\n",
    "for offset in range(0, 3):\n",
    "    url = base_url + str(offset * 60)\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, \"html.parser\")\n",
    "    table_body = soup.find(\"tbody\")\n",
    "    base = \"https://sofifa.com/\"\n",
    "    for row in table_body.findAll(\"tr\"):\n",
    "        p_link = tr.select('td.col-name')[0].find(\"a\").get(\"href\")\n",
    "        p_url = base + p_link\n",
    "        td = row.findAll(\"td\")\n",
    "        pid = td[0].find(\"img\").get(\"id\")\n",
    "        nationality = td[1].find(\"img\").get(\"title\")\n",
    "        name = td[1].find(\"a\").get(\"aria-label\")\n",
    "        rel = td[1].findAll(\"a\", {\"rel\" : \"nofollow\"})\n",
    "        pos = rel[0].findAll(\"span\")\n",
    "        for span in pos:\n",
    "            positions= (span.text.split) ## FIX HERE\n",
    "        age = td[2].text\n",
    "        overall = td[3].text.strip()\n",
    "        potential = td[4].text.strip()\n",
    "        club = td[5].find(\"a\").text\n",
    "        value = td[6].text.strip()\n",
    "        wage = td[7].text.strip()\n",
    "        # store the retrieved \n",
    "        player_data = pd.DataFrame([[pid, name, age, positions,nationality, overall,potential, club,value,wage]])\n",
    "        player_data.columns = columns\n",
    "        data = pd.concat([data, player_data])\n",
    "    print(\"done for \"+str(offset), end = \"\\r\")\n",
    "    p_urls.append(format(p_url))\n",
    "data.drop_duplicates()\n",
    "data.head()\n",
    "data.to_csv('../data/player_data.csv', encoding='utf-8-sig')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76f389-1e02-40dc-b6f8-dbbafeb9acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to deep scrape info (NOT DONE) check later)\n",
    "# You need to first go surface to scrap and get new url within page \n",
    "# of the players and start scraping from there\n",
    "# So might a different lists or dictionaries to store new urls\n",
    "# And finish scarping it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d318e2a2-3620-40b0-a847-5062fe9673f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m offset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      5\u001b[0m     url \u001b[38;5;241m=\u001b[39m base_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(offset \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m      7\u001b[0m     text \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m      8\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "base = \"https://sofifa.com/\"\n",
    "base_url = \"https://sofifa.com/players?offset=\"\n",
    "p_urls = []\n",
    "for offset in range(0, 5):\n",
    "    url = base_url + str(offset * 60)\n",
    "    res = requests.get(url)\n",
    "    text = res.text\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    tab_body = soup.find(\"tbody\")\n",
    "    trs = tab_body.findAll(\"tr\")\n",
    "    for tr in trs2:\n",
    "        link = tr.select('td.col-name')[0].find(\"a\").get(\"href\")\n",
    "        p_url = base + link\n",
    "        if (p_url not in p_urls):\n",
    "            p_urls.append(format(p_url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e27c897-eac0-4f22-9024-b9ba98cf6587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://sofifa.com//player/246618/adam-hlozek/220051/'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_body = soup.find(\"tbody\")\n",
    "trs2 = tab_body.findAll(\"tr\")\n",
    "for tr in trs2:\n",
    "    a = tr.select('td.col-name')[0].find(\"a\").get(\"href\")\n",
    "base + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5250980f-e6df-4cae-8cb0-0038d823b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#att_block = stats_block[0]\n",
    "#skill_block = stats_block[1]\n",
    "#movement_block = stats_block[2]\n",
    "#power_block = stats_block[3]\n",
    "#mentality_block = stats_block[4]\n",
    "#defending_block = stats_block[5]\n",
    "#goalkeeping_block = stats_block[6]\n",
    "\n",
    "#tr = skill_block\n",
    "#tr.findAll(\"li\")[0]\n",
    "#stats.append([extract_stats(tr) for tr in trs])\n",
    "\n",
    "# Convert list of lists to single list\n",
    "#flatten = lambda x: list(chain.from_iterable(x))\n",
    "\n",
    "# Convert to df\n",
    "# df = pd.DataFrame(flatten(stats))\n",
    "# df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "# df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
